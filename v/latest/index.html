<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Wouter Saelens *" />
  <meta name="author" content="Robrecht Cannoodt *" />
  <meta name="author" content="Lukas Weber" />
  <meta name="author" content="Charlotte Soneson" />
  <meta name="author" content="Yvan Saeys *" />
  <meta name="author" content="Mark D. Robinson *" />
  <meta name="dcterms.date" content="2018-11-19" />
  <meta name="keywords" content="bioinformatics, methods, benchmarking, single-cell analysis" />
  <title>A workflow for continuous and collaborative benchmarking</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="github-pandoc.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <!-- Insert Analytics Script Below -->
  <script>
  </script>
  <!-- End Analytics Script -->
</head>
<body>
<header id="title-block-header">
<h1 class="title">A workflow for continuous and collaborative benchmarking</h1>
</header>
<p><small><em> This manuscript (<a href="https://komparo.github.io/manuscript-workflow/v/92df67c5b19bb1ff6b227d807b56e2a3832aa64e/">permalink</a>) was automatically generated from <a href="https://github.com/komparo/manuscript-workflow/tree/92df67c5b19bb1ff6b227d807b56e2a3832aa64e">komparo/manuscript-workflow@92df67c</a> on November 19, 2018. </em></small></p>
<h2 id="authors">Authors</h2>
<ul>
<li><p><strong>Wouter Saelens </strong>*<br> <img src="images/orcid.svg" alt="ORCID icon" width="13" height="13" /> <a href="https://orcid.org/0000-0002-7114-6248">0000-0002-7114-6248</a> · <img src="images/github.svg" alt="GitHub icon" width="13" height="13" /> <a href="https://github.com/zouter">zouter</a> · <img src="images/twitter.svg" alt="Twitter icon" width="13" height="13" /> <a href="https://twitter.com/zouters">zouters</a><br> <small> Data Mining and Modelling for Biomedicine, VIB Center for Inflammation Research, Ghent, Belgium; Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Ghent, Belgium · Funded by Fonds Wetenschappelijk Onderzoek </small></p></li>
<li><p><strong>Robrecht Cannoodt </strong>*<br> · <img src="images/github.svg" alt="GitHub icon" width="13" height="13" /> <a href="https://github.com/rcannood">rcannood</a> · <img src="images/twitter.svg" alt="Twitter icon" width="13" height="13" /> <a href="https://twitter.com/rcannood">rcannood</a><br> <small> Data Mining and Modelling for Biomedicine, VIB Center for Inflammation Research, Ghent, Belgium; Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Ghent, Belgium · Funded by Fonds Wetenschappelijk Onderzoek </small></p></li>
<li><p><strong>Lukas Weber</strong><br> · <img src="images/twitter.svg" alt="Twitter icon" width="13" height="13" /> <a href="https://twitter.com/lmwebr">lmwebr</a><br> <small> Institute of Molecular Life Sciences, University of Zurich, Zurich, Switzerland; SIB Swiss Institute of Bioinformatics, University of Zurich, Zurich, Switzerland </small></p></li>
<li><p><strong>Charlotte Soneson</strong><br> · <img src="images/twitter.svg" alt="Twitter icon" width="13" height="13" /> <a href="https://twitter.com/CSoneson">CSoneson</a><br> <small> Institute of Molecular Life Sciences, University of Zurich, Zurich, Switzerland; SIB Swiss Institute of Bioinformatics, University of Zurich, Zurich, Switzerland; Friedrich Miescher Institute for Biomedical Research, Basel, Switzerland </small></p></li>
<li><p><strong>Yvan Saeys </strong>*<br><br> <small> Data Mining and Modelling for Biomedicine, VIB Center for Inflammation Research, Ghent, Belgium; Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Ghent, Belgium </small></p></li>
<li><p><strong>Mark D. Robinson </strong>*<br> · <img src="images/twitter.svg" alt="Twitter icon" width="13" height="13" /> <a href="https://twitter.com/markrobinsonca">markrobinsonca</a><br> <small> Institute of Molecular Life Sciences, University of Zurich, Zurich, Switzerland; SIB Swiss Institute of Bioinformatics, University of Zurich, Zurich, Switzerland </small></p></li>
</ul>
<h2 id="abstract" class="page_break_before">Abstract</h2>
<p>Benchmarking is a critical step in the development of bioinformatics tools, but the way benchmarking is done at the moment has some limitations. Because each benchmark is developed in isolation, they tend to be hard to compare, extend and are rapidly outdated. Moreover, benchmarks are usually rapidily outdated as new methods are developed. To address these challenges, we combined modern software development tools to create a workflow for continuous and collaborative benchmarking. The structure of the benchmark is highly modular, so that anyone can contribute a set of datasets, metrics, methods or interprete the results, and get credit for their contributions. As a test case, we applied this worklow on an emerging type of analysis in the single-cell field: trajectory differential expression, available at <a href="https://github.com/komparo/tde">https://github.com/komparo/tde</a>. A skeleton version of the workflow, which can be used to create a similar benchmarking workflow for a different type of methods, can be found at <a href="https://github.com/komparo/skeleton">https://github.com/komparo/skeleton</a>.</p>
<h2 id="introduction">Introduction</h2>
<!-- Benchmarking ---------------------------------------------------------------------------------------------------->
<p>Evaluating the performance of a new method, and comparing it to the state-of-the-art, is a critical step in the development of bioinformatics methods. Benchmarks are essential to showcase the advantages and weaknesses of a method, and assure that new tools improve upon related methods. Despite this, well-designed and balanced benchmarking strategy can be difficult to create, especially when a ground truth on real data is not available.</p>
<p>The breadth of a benchmark is influenced by its purpose. In some studies, the goal is to review the methods available in the field, and highlight current challenges. Such independent benchmarks are usually very comprehensive, involving many datasets and different metrics ranging assessing the accuracy, scalability and robustness of a method. A special case of such a benchmark are competitions, where the focus lies on promoting the development of new methods within the field, while using existing methods as baseline. Other benchmarks are used as a companion to a study proposing a new method, demonstrating its improvements and usefulness.</p>
<!-- Problem setting ---------------------------------------------------------------------------------------------------->
<p>While benchmarks are unmistakingly important, the way benchmarking is usually done has some limitations:</p>
<ul>
<li>Benchmarks are quickly outdated when new methods come along.</li>
<li>Benchmarks are difficult to extend, as this is usually only added as an afterthought. <!-- examples: @6VtYRDAK --></li>
<li>While benchmarks often reach different conclusions, they are difficult to compare, because of (unclear) differences in datasets, method parameters, metric implementation and aggregation.</li>
<li>Independent benchmarks and competitions tend to be authoritative, with only a small group of people deciding on how methods should be compared.</li>
<li>Independent benchmarks are usually published quite late, only after a lot of methods are already available.</li>
<li>Companion benchmarks represent in some way a lot of wasted effort, because datasets are often reanalysed, metrics reimplemented, and methods rewrapped.</li>
</ul>
<!---
Only the most relevant limitations are discussed here, others:

* Easy to overfit on what is available / ideas of the benchmarker. Examples include the Trapnell trajectory datasets, MINST dataset, but there are several others to be found. At the same time, it is also useful to only have a limited number of "reference" datasets available, to make it easier to develop new methods, so this makes this limitation somewhat controversial. In any case, the datasets should be a good representation of what the method should find "in the wild".
* Companion benchmarks always come down to the same thing.

--->
<!-- Goals  --------------------------------------------------------------------------------------------------------------->
<p>To resolve these issues, we created a workflow for benchmarking which centers around the following three core concepts:</p>
<ul>
<li><strong>Modular</strong>: It should be possible to extend the benchmark simply by adding a self-contained “module”. Such a module could be: a dataset generator, a method, a set of metrics, or a report generator that interpretes the results and produces a report. Several tools exist already for making benchmarks modular: SummarizedBenchmark <span class="citation" data-cites="Yqv1RKnE">[<a href="#ref-Yqv1RKnE">1</a>]</span>, <a href="https://github.com/stephens999/dscr">Dynamic Statistical Comparisons</a> and iCOBRA <span class="citation" data-cites="Bqyk8BrN">[<a href="#ref-Bqyk8BrN">2</a>]</span>. <!--- TODO #1 ---></li>
<li><strong>Collaborative</strong>: Anyone with a computer and internet connection should be able to run and contribute to the benchmark. This can range from contributing a module, to changing the structure of the benchmark itself. Discussions on the benchmark or any of the reports should also be open. The collaborative aspect of benchmarking has usually focused on the level of methods, with countless competitions and challenges, such as those organised by <a href="http://dreamchallenges.org/">DREAM</a> or <a href="https://www.kaggle.com/">kaggle</a>.</li>
<li><strong>Continuous</strong>: A benchmark should be continously updated when new modules are added. This has quite a long history in bioinformatics, particularly in structure prediction <span class="citation" data-cites="bxALj0Vw">[<a href="#ref-bxALj0Vw">3</a>]</span>, but also in other fields <span class="citation" data-cites="17EgWn4e7">[<a href="#ref-17EgWn4e7">4</a>]</span>. <!---- TODO #2 ----></li>
</ul>
<p>To construct a workflow which fulfills combines these three concepts, we used several ideas and tools coming from modern software development, such as continuous integration, containerisation and workflow management.</p>
<!-- General overview ------------------------------------------------------------------------------------------------------->
<p>In brief, our workflow is structured as follows. We define several different <strong>types of modules</strong> (Figure <a href="#fig:overview">1</a>a): dataset generators can generate datasets and optionally use another dataset as input, methods use a dataset to generate some model, metrics will calculate some scores using the model and optionally also parts of the dataset, and finally a report generator which summarise the datasets, models and scores into a report. Each type of module can generate a set of files which are constrained to a particular set of <strong>formats</strong> (Figure <a href="#fig:overview">1</a>b). Each format has an unambiguous description, a set of good and bad examples, and includes a validator which validates the output files generated by each module. While each format is defined beforehand, new formats can be added over time as the field progresses. A <strong>module</strong> (Figure <a href="#fig:overview">1</a>c) is a set of scripts and packages, which are run inside a portable environment. This module is put under version control, shared on a code sharing platform, and tested automatically using continuous integration. When all tests of a module are succesful, these modules can be integrated into the actual <strong>benchmarking workflow</strong> (Figure <a href="#fig:overview">1</a>d). Within this workflow, modules are connected through a particular design, which is executed using a workflow manager. The output of the benchmark are a set of reports and apps, which are made available through a publishing platforms. To add a new module, a pull request is created to integrate the module within the benchmarking workflow, after which the contribution is reviewed openly. When accepted, the module is automatically integrated within the workflow, and the necessary parts of the workflow are re-executed. Finally, in regular time intervals (e.g. monthly), the full set of reports and apps are gathered and versioned.</p>
<!-- Test case ------------------------------------------------------------------------------------------------------------->
<p>As a test case, we developed a proof-of-concept benchmark for single-cell trajectory differential expression (TDE) methods. TDE methods try to find genes which are differentially expressed along a trajectory, the latter of which is an positioning of cells along a graph structure. Given that only a few of such methods have been developed yet <span class="citation" data-cites="13yVWlTc9 HkU0XKEf 16zLRyIvX">[<a href="#ref-13yVWlTc9">5</a>,<a href="#ref-HkU0XKEf">6</a>,<a href="#ref-16zLRyIvX">7</a>]</span>, this is the ideal scenario for developing the idea of a continuous and collaborative benchmark, and try to find solutions to the inevitable challenges which will come up as the field develops.</p>
<p>We will further discuss each element of the workflow in detail, along with how we currently implemented it in practice. It is important to acknowledge here that this is only one possible implementation, and that other tools, some of which still have to be developed, could better fit the benchmarking use-case. What is the most important is not the way the benchmark is implemented, but the ideas behind its implementation.</p>
<figure>
<img src="images/pipeline.png" alt="Figure 1: The pipeline." id="fig:overview" style="width:100.0%" /><figcaption><span>Figure 1:</span> The pipeline.</figcaption>
</figure>
<h2 id="data-formats">Data formats</h2>
<p>The basis of any collaborative effort in computation biology is agreeing on how data will be interchanged, and benchmarking is no exception. Sometimes, the differences between data formats can be minor, for example whether the samples within a gene expression matrix are put in the rows or in the column. In other cases, different data formats can have a significant impact on storage and/or the speed by which the data can be processed. <!--- TODO #3 ---></p>
<p>In the benchmarking workflow presented here, a format represents how a particular part of a dataset, model, score or report should be represented on disk.</p>
<h3 id="what-a-format-entails">What a format entails</h3>
<!-- Description ------------------------------------------------------------------------------------------------------------->
<p>For a format to be useful, it should have an unambiguous description. In this way, someone developing a module can be sure how the inputs look like, even if these inputs do not exist yet, and can also be sure that the outputs will be useful as input for other modules.</p>
<!-- Validation --------------------------------------------------------------------------------------------------------------->
<p>While a description is meant to be readable by humans, this description should also be translated into a lanuage computers can understand, so that each data file produced by a module can be validated. In this way, developers of a module can get immediate feedback on whether their output matches the format description. To make a format validatable, it is one possibility to use one of the many “schemas” available, such as json-schema (<a href="http://json-schema.org/">http://json-schema.org/</a>), XML schemas or Apache Arrow Schemas. Often, there are already validators available for these schemas (json-schemas for example: <a href="https://json-schema.org/implementations.html#validators">https://json-schema.org/implementations.html#validators</a>). On the other hand, for more custom file formats or complex behaviour, we will have to implement the validator ourselves.</p>
<!-- Examples ----------------------------------------------------------------------------------------------------------------->
<p>Finally, connecting the human-readable description with the computation validation can be done with providing good and bad examples of the data format. These examples serve a double purpose, because they provide the module contributors several examples as a help to understand the description, but can also be used as test cases for the format validators.</p>
<h3 id="formats-change-as-the-field-progresses">Formats change as the field progresses</h3>
<p>Usually, the most optimal representation of a dataset or the output of a method only becomes apparent when several methods have been developed already. This means that any effort to make a benchmark collaborative and continuous should strive to make its data formats flexible. Flexibility can take several forms. New features could be added to the format, without invalidating the old data and modules. When this is not an option, new formats could be added alongside the old. When applicable, converters should then be written which convert the old formats into the new, so that old modules keep on functioning. Finally, in some extreme cases, old formats could be invalidated and replaced with new formats, which would require some versioning system to make sure modules are run on the version of the formats they were developed.</p>
<p>It is inevitable that disagreements about data representation will pop up in a collaborative effort. But in any case, having common formats, even if they are suboptimal for certain use cases, is usually better then having none at all.</p>
<h3 id="test-case">Test case</h3>
<p>For the TDE use case, to keep the formats simple and accesible, we decided to mainly use text-based formats such as comma-separated values (CSV) and JSON files, as these can be rapidly parsed in almost any programming language. For each format, we wrote custom validators in R, although JSON files were also partly validated using a json schema validator (ajv, <a href="https://ajv.js.org/">https://ajv.js.org/</a>). These validators are available as an R package (<a href="https://github.com/komparo/tde_formats">https://github.com/komparo/tde_formats</a>). For each format, we also wrote a description and several examples, which are shown together in the contributor’s guide (<a href="https://komparo.github.io/tde/formats.html">https://komparo.github.io/tde/formats.html</a>).</p>
<p>To represent a dataset and a model, we needed to define several formats grouped within three categories:</p>
<ul>
<li>Single-cell expression datasets: This is relatively straightforward, given that the sheer number of single-cell RNA-seq tools available. As is commonly done, we represent the dataset as a matrix, together with two tables containing the metadata of the cells and features respectively.</li>
<li>Trajectory: This is more difficult, given the vast diversity in outputs of trajectory inference methods. In our recent benchmarking of these methods, we already created a common format for trajectories, and included this format in this benchmark as well. However, as we also acknowledge in that study, alternative formats may be possible.</li>
<li>Trajectory differential expression: This is the the most difficult, because we had to make some educated guesses about how we expect the field to progress. While it impossible to know beforehand what formats will be ultimately needed, we can at least try to already predict for them. Based on what is already done by some existing methods, we defined several types of trajectory differential expression such as oscillatory (within a cyclical trajectory), pseudotime (expression that changes along pseudotime), and branch point (expression that happens at a particular branch point). We extended this with some other types of differential expression, such as local (changes in expression at a particular point of the trajectory) and overall (changes in expression anywhere in the trajectory). Each kind of differential expression is represented in a table format, where each differential expression event is represented in a row which can contain information on the associated p-value, a ranking and/or effect size.</li>
</ul>
<p>Scores and reports have less requirements regarding the formats. The most simplest format to represent a score is as an atomic value, in which each model can be summarized by one quantitave or qualitative value. Of course, more complex score formats are also possible. For reports, we included two formats. A static report with at least an index HTML or markdown file, together with any extra assets such as figures. A dynamic report on the other hand produces a software container which will expose a port serving a web app when ran.</p>
<h2 id="module-types">Module types</h2>
<p>In the benchmarking workflow, we define four types of modules. In our experience, these four modules are in most cases enough to construct a fully comprehensive benchmark of certain group of methods.</p>
<h3 id="dataset-generators">Dataset generators</h3>
<p>This module will generate a dataset, which can from very simple “toy” data, synthetic data which try to mimic the characteristics of real data as best as possible, or real datasets. Optionally, a dataset generator can also use another dataset as input, for example when generating some synthetic data based on some real data. Only rarely will a dataset generator contain some primary data itself. Rather, data should be gathered directly from primary sources, for example using APIs by various databases or by downloading the data directly from data management systems such as Zenodo or Figshare.</p>
<p>Within the TDE test case, we included</p>
<h3 id="methods">Methods</h3>
<p>A method module reads in (parts) of a dataset, and uses this to generate a model. Some special types of methods can be helpful to include at the start of a benchmark. Positive controls, for example a method that simply return the reference model of the dataset, and negative controls, for example a method which generate a random model, could be useful to make sure the metrics work correctly. <em>Off-the-shelf</em> methods, methods that can be easily implemented with just a few lines of code, could be helpful as a baseline to other methods and to assess the difficulty of particular datasets.</p>
<h3 id="metrics">Metrics</h3>
<p>Metric modules score the output of a model. Some metrics assess the accuracy of a model by comparing it with some reference model present in the dataset. Others will look at the resources consumed by the method, such as CPU time and memory, to assess its scalability. Models can also be compared to other models, for example to examine the stability of a method. Finally, some qualitative metrics can also be defined here, for example those that look at the usability of a method.</p>
<h3 id="report-generators">Report generators</h3>
<p>In the end, the scores are aggregated and interpreted using a report generator. This modules generates a report which can be static, such as a markdown document with some figures, or dynamic in the form of a web application. By crowdsourcing the interpretation of the benchmark in this way, it would become much less authorative and instead promote open discussion in the field <span class="citation" data-cites="YsUprDyN">[<a href="#ref-YsUprDyN">8</a>]</span>. It might certainly happen that different reports would contain contradicting results, but because each reports starts from the same set of data, it would be immediatly clear why the conclusions differ. For example, there might be subtle differences in how the scores have been averaged. Or, a report may only have focused on only a subset of the dataset which the authors found the most relevant for their method.</p>
<h2 id="modules">Modules</h2>
<h3 id="scripts-a-portable-environment-and-metadata">Scripts, a portable environment and metadata</h3>
<p>A module needs to contain at least command, which will run some code that reads in the input data, process it in some way, and ultimately write the output data in the correct format.</p>
<p>Given the large diversity of programming languages used in computation biology, a collaborative benchmarking effort should try to avoid imposing limits on the kind of programming languages that can be used. As an example, the single-cell analysis field is split between tools written for R and Python <span class="citation" data-cites="ekkzy8ZR">[<a href="#ref-ekkzy8ZR">9</a>]</span>, and choosing one of these two would therefore alienate a signficant part of the field. Moreover, a collaborative effort should also be open for new languages such as Julia <span class="citation" data-cites="1B6CyO0CV">[<a href="#ref-1B6CyO0CV">10</a>]</span>, which could be more powerful and developer friendly for certain use cases.</p>
<p>Apart from being language agnostic, the execution of a module should also happen on any computer in exactly the same way. To make the execution reproducible, we therefore require that a module defines a portable environment, which contains the necessary operating system, language interpreters and other packages to execute the code within the module. An environment can be portable on many levels: within one programming language such as virtualenv for python or packrat for R or across languages using package managers such as Conda. The most complete level of reproducibility can be obtained by working at the level of the operating system, through container systems such as docker or singularity. Finally, to be able execute stochastic code in a reproducible manner, it is also necessary to fix the pseudo-random number generator in some way, we do this by always setting an a priori defined seed through R or numpy.</p>
<p>A module also contains metadata, which lists the requirements to run the method such as the inputs, outputs and the name of the portable environment. Within our workflow, we also require some data for organisational purposes, such as a list of authors with their contributions, and the license of the code within the module.</p>
<h3 id="version-control-and-code-sharing">Version control and code sharing</h3>
<p>We require that the complete module, including the portable environment and metadata, is placed under version control so that any changes are tracked. The module is then shared on a code sharing platform, which makes it possible for other module authors and maintainers of the benchmark to file issues on the module, request some changes to the code through pull requests, and create a modifications if the license allows it. In our workflow, we use git for version control and GitHub as the platform to share modules, although it should be noted that powerful variants of the latter exist, including self-hosted ones.</p>
<h3 id="continuous-integration">Continuous integration</h3>
<p>To keep the development of a module and benchmark maintainable, it is important that each element of the module is automatically tested and validated. In this way, many errors are catched early, before they can impact other modules in the benchmark. Including automated testing also reduces the burden for those reviewing the modules. This crosstalk between automated testing and manual reviewing is already commonplace in many package repositories, such as CRAN and Bioconductor.</p>
<p>In our proposed workflow, we automatically trigger a new test on <a href="https://www.travis-ci.com">travis-ci.com</a>, which is free for open-source projects. We test each module on several levels. We first check whether it contains all required content, and whether the metadata is complete. Next, we activate the portable environment, run the module on some small input data, and validate the produced output. If any of these steps fail, the author is notified. Only when tests are sucessful can the new module be integrated into the whole benchmark procedure.</p>
<h2 id="combining-modules-within-a-benchmark">Combining modules within a benchmark</h2>
<p>In principle, it should be possible for anyone to extend or adapt the benchmark for their own purposes. At the same, it is also important to have a central place which lists all the modules and provides the most up-to-date set of reports for interested readers. To make this possible, our benchmarking workflow has one “main” repository, which lists the location of the different modules and how they are combined in the benchmark. Anyone can however create a fork of this repository, adapt the modules or benchmarking design in any way, and run it using their own infrastructure.</p>
<h3 id="executing-the-benchmark">Executing the benchmark</h3>
<p>For the execution of the modules, a pipeline manager such as snakemake <span class="citation" data-cites="NcYZqBux">[<a href="#ref-NcYZqBux">11</a>]</span> or nextflow <span class="citation" data-cites="4XDvZWxk">[<a href="#ref-4XDvZWxk">12</a>]</span> is almost indispensable. These tools make sure the modules are executed in the correct order and within a reproducible environment. Moreover, to make the benchmark scalable, a pipeline manager will only rerun those exeuctions for which any inputs have changed, which includes changes to any scripts or packages inside the portable environment. Within our benchmarking workflow, we created a custom pipeline manager for this, which provided us with some features that are lacking in most current pipeline managers, such as incrementality at the level of the portable environment, output validation and fixation of the pseudo-random number generator.</p>
<h3 id="pipeline-manager">Pipeline manager</h3>
<h3 id="execution">Execution</h3>
<h3 id="continuous-publishing">Continuous publishing</h3>
<h3 id="adding-or-updating-a-module">Adding or updating a module</h3>
<h3 id="continuous-integration-1">Continuous integration</h3>
<h3 id="versioning">Versioning</h3>
<h2 id="outlook">Outlook</h2>
<p>The project as it stands now is meant to be a proof-of-concept. Technologies, and the companies and communities building them, come and go, and the tools we used for this benchmark will almost certainly feel outdated in a couple of years. The crucial point is not which tools are used, but what advantages they provide for the community: a portable environment, a reproducible workflow, a way to collaboratively design a benchmark, and ultimately a more democratic view of the field and its challenges lying ahead.</p>
<p>In the ideal case, a continuous benchmarking project should be supported by a larger consortium, such as the Human Cell Atlas, which would not only assure its continuity, but would also provide infrastructure support. In particular, services which have strong requirements on the side of storage and/or computing power would benefit from this, such as continuous integration, the environment registry, and the execution cluster.</p>
<p>By providing a shared platform where old and current ideas are rigorously tested, and new ideas can be easily validated,</p>
<p>A platform like this should be build upon the idea that future methods and output formats can never be predicted, but at least we can prepare for them.</p>
<h3 id="reports-as-a-forum">Reports as a forum</h3>
<ul>
<li>Discuss multiple possible interpretations</li>
<li>Self-assessment trap <span class="citation" data-cites="9jjMfS7z">[<a href="#ref-9jjMfS7z">13</a>]</span></li>
</ul>
<h2 id="references" class="page_break_before">References</h2>
<!-- Explicitly insert bibliography here -->
<div id="refs">
<div id="ref-Yqv1RKnE">
<p>1. <strong>Reproducible and replicable comparisons using SummarizedBenchmark</strong><br />
Patrick K Kimes, Alejandro Reyes<br />
<em>Bioinformatics</em> (2018-07-17) <a href="https://doi.org/gdvt5p">https://doi.org/gdvt5p</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/bty627">10.1093/bioinformatics/bty627</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/30016409">30016409</a></p>
</div>
<div id="ref-Bqyk8BrN">
<p>2. <strong>iCOBRA: open, reproducible, standardized and live method benchmarking</strong><br />
Charlotte Soneson, Mark D Robinson<br />
<em>Nature Methods</em> (2016-04) <a href="https://doi.org/gfj2zx">https://doi.org/gfj2zx</a><br />
DOI: <a href="https://doi.org/10.1038/nmeth.3805">10.1038/nmeth.3805</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/27027585">27027585</a></p>
</div>
<div id="ref-bxALj0Vw">
<p>3. <strong>Critical assessment of methods of protein structure prediction (CASP)-Round XII</strong><br />
John Moult, Krzysztof Fidelis, Andriy Kryshtafovych, Torsten Schwede, Anna Tramontano<br />
<em>Proteins: Structure, Function, and Bioinformatics</em> (2017-12-15) <a href="https://doi.org/gfj2zw">https://doi.org/gfj2zw</a><br />
DOI: <a href="https://doi.org/10.1002/prot.25415">10.1002/prot.25415</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/29082672">29082672</a> · PMCID: <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5897042">PMC5897042</a></p>
</div>
<div id="ref-17EgWn4e7">
<p>4. <strong>A benchmark for RNA-seq quantification pipelines</strong><br />
Mingxiang Teng, Michael I. Love, Carrie A. Davis, Sarah Djebali, Alexander Dobin, Brenton R. Graveley, Sheng Li, Christopher E. Mason, Sara Olson, Dmitri Pervouchine, … Rafael A. Irizarry<br />
<em>Genome Biology</em> (2016-04-23) <a href="https://doi.org/gfj2zz">https://doi.org/gfj2zz</a><br />
DOI: <a href="https://doi.org/10.1186/s13059-016-0940-1">10.1186/s13059-016-0940-1</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/27107712">27107712</a> · PMCID: <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4842274">PMC4842274</a></p>
</div>
<div id="ref-13yVWlTc9">
<p>5. <strong>A descriptive marker gene approach to single-cell pseudotime inference</strong><br />
Kieran R. Campbell, Christopher Yau<br />
<em>Cold Spring Harbor Laboratory</em> (2016-06-23) <a href="https://doi.org/gfj23j">https://doi.org/gfj23j</a><br />
DOI: <a href="https://doi.org/10.1101/060442">10.1101/060442</a></p>
</div>
<div id="ref-HkU0XKEf">
<p>6. <strong>SCORPIUS improves trajectory inference and identifies novel modules in dendritic cell development</strong><br />
Robrecht Cannoodt, Wouter Saelens, Dorine Sichien, Simon Tavernier, Sophie Janssens, Martin Guilliams, Bart N Lambrecht, Katleen De Preter, Yvan Saeys<br />
<em>Cold Spring Harbor Laboratory</em> (2016-10-06) <a href="https://doi.org/gfj23n">https://doi.org/gfj23n</a><br />
DOI: <a href="https://doi.org/10.1101/079509">10.1101/079509</a></p>
</div>
<div id="ref-16zLRyIvX">
<p>7. <strong>Reversed graph embedding resolves complex single-cell trajectories</strong><br />
Xiaojie Qiu, Qi Mao, Ying Tang, Li Wang, Raghav Chawla, Hannah A Pliner, Cole Trapnell<br />
<em>Nature Methods</em> (2017-08-21) <a href="https://doi.org/gc5v2g">https://doi.org/gc5v2g</a><br />
DOI: <a href="https://doi.org/10.1038/nmeth.4402">10.1038/nmeth.4402</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/28825705">28825705</a> · PMCID: <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5764547">PMC5764547</a></p>
</div>
<div id="ref-YsUprDyN">
<p>8. <strong>Crowdsourced research: Many hands make tight work</strong><br />
Raphael Silberzahn, Eric L. Uhlmann<br />
<em>Nature</em> (2015-10-07) <a href="https://doi.org/gc4ntn">https://doi.org/gc4ntn</a><br />
DOI: <a href="https://doi.org/10.1038/526189a">10.1038/526189a</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/26450041">26450041</a></p>
</div>
<div id="ref-ekkzy8ZR">
<p>9. <strong>Exploring the single-cell RNA-seq analysis landscape with the scRNA-tools database</strong><br />
Luke Zappia, Belinda Phipson, Alicia Oshlack<br />
<em>PLOS Computational Biology</em> (2018-06-25) <a href="https://doi.org/gdqcjz">https://doi.org/gdqcjz</a><br />
DOI: <a href="https://doi.org/10.1371/journal.pcbi.1006245">10.1371/journal.pcbi.1006245</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/29939984">29939984</a> · PMCID: <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6034903">PMC6034903</a></p>
</div>
<div id="ref-1B6CyO0CV">
<p>10. <strong>Julia: A Fresh Approach to Numerical Computing</strong><br />
Jeff Bezanson, Alan Edelman, Stefan Karpinski, Viral B. Shah<br />
<em>arXiv</em> (2014-11-06) <a href="https://arxiv.org/abs/1411.1607v4">https://arxiv.org/abs/1411.1607v4</a></p>
</div>
<div id="ref-NcYZqBux">
<p>11. <strong>Snakemake–a scalable bioinformatics workflow engine</strong><br />
J. Koster, S. Rahmann<br />
<em>Bioinformatics</em> (2012-08-20) <a href="https://doi.org/gd2xzq">https://doi.org/gd2xzq</a><br />
DOI: <a href="https://doi.org/10.1093/bioinformatics/bts480">10.1093/bioinformatics/bts480</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/22908215">22908215</a></p>
</div>
<div id="ref-4XDvZWxk">
<p>12. <strong>Nextflow enables reproducible computational workflows</strong><br />
Paolo Di Tommaso, Maria Chatzou, Evan W Floden, Pablo Prieto Barja, Emilio Palumbo, Cedric Notredame<br />
<em>Nature Biotechnology</em> (2017-04) <a href="https://doi.org/gfj52z">https://doi.org/gfj52z</a><br />
DOI: <a href="https://doi.org/10.1038/nbt.3820">10.1038/nbt.3820</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/28398311">28398311</a></p>
</div>
<div id="ref-9jjMfS7z">
<p>13. <strong>The self-assessment trap: can we all be better than average?</strong><br />
R. Norel, J. J. Rice, G. Stolovitzky<br />
<em>Molecular Systems Biology</em> (2014-04-16) <a href="https://doi.org/bxxmvz">https://doi.org/bxxmvz</a><br />
DOI: <a href="https://doi.org/10.1038/msb.2011.70">10.1038/msb.2011.70</a> · PMID: <a href="http://www.ncbi.nlm.nih.gov/pubmed/21988833">21988833</a> · PMCID: <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3261704">PMC3261704</a></p>
</div>
</div>
<script>
// AnchorJS minified version below.
// Source https://github.com/bryanbraun/anchorjs/blob/064abdd0987f305933ec4982af6d0c1cf2fd0814/anchor.js

/**
 * AnchorJS - v4.0.0 - 2017-06-02
 * https://github.com/bryanbraun/anchorjs
 * Copyright (c) 2017 Bryan Braun; Licensed MIT
 */
!function(A,e){"use strict";"function"==typeof define&&define.amd?define([],e):"object"==typeof module&&module.exports?module.exports=e():(A.AnchorJS=e(),A.anchors=new A.AnchorJS)}(this,function(){"use strict";function A(A){function e(A){A.icon=A.hasOwnProperty("icon")?A.icon:"",A.visible=A.hasOwnProperty("visible")?A.visible:"hover",A.placement=A.hasOwnProperty("placement")?A.placement:"right",A.class=A.hasOwnProperty("class")?A.class:"",A.truncate=A.hasOwnProperty("truncate")?Math.floor(A.truncate):64}function t(A){var e;if("string"==typeof A||A instanceof String)e=[].slice.call(document.querySelectorAll(A));else{if(!(Array.isArray(A)||A instanceof NodeList))throw new Error("The selector provided to AnchorJS was invalid.");e=[].slice.call(A)}return e}function n(){if(null===document.head.querySelector("style.anchorjs")){var A,e=document.createElement("style");e.className="anchorjs",e.appendChild(document.createTextNode("")),void 0===(A=document.head.querySelector('[rel="stylesheet"], style'))?document.head.appendChild(e):document.head.insertBefore(e,A),e.sheet.insertRule(" .anchorjs-link {   opacity: 0;   text-decoration: none;   -webkit-font-smoothing: antialiased;   -moz-osx-font-smoothing: grayscale; }",e.sheet.cssRules.length),e.sheet.insertRule(" *:hover > .anchorjs-link, .anchorjs-link:focus  {   opacity: 1; }",e.sheet.cssRules.length),e.sheet.insertRule(" [data-anchorjs-icon]::after {   content: attr(data-anchorjs-icon); }",e.sheet.cssRules.length),e.sheet.insertRule(' @font-face {   font-family: "anchorjs-icons";   src: url(data:n/a;base64,AAEAAAALAIAAAwAwT1MvMg8yG2cAAAE4AAAAYGNtYXDp3gC3AAABpAAAAExnYXNwAAAAEAAAA9wAAAAIZ2x5ZlQCcfwAAAH4AAABCGhlYWQHFvHyAAAAvAAAADZoaGVhBnACFwAAAPQAAAAkaG10eASAADEAAAGYAAAADGxvY2EACACEAAAB8AAAAAhtYXhwAAYAVwAAARgAAAAgbmFtZQGOH9cAAAMAAAAAunBvc3QAAwAAAAADvAAAACAAAQAAAAEAAHzE2p9fDzz1AAkEAAAAAADRecUWAAAAANQA6R8AAAAAAoACwAAAAAgAAgAAAAAAAAABAAADwP/AAAACgAAA/9MCrQABAAAAAAAAAAAAAAAAAAAAAwABAAAAAwBVAAIAAAAAAAIAAAAAAAAAAAAAAAAAAAAAAAMCQAGQAAUAAAKZAswAAACPApkCzAAAAesAMwEJAAAAAAAAAAAAAAAAAAAAARAAAAAAAAAAAAAAAAAAAAAAQAAg//0DwP/AAEADwABAAAAAAQAAAAAAAAAAAAAAIAAAAAAAAAIAAAACgAAxAAAAAwAAAAMAAAAcAAEAAwAAABwAAwABAAAAHAAEADAAAAAIAAgAAgAAACDpy//9//8AAAAg6cv//f///+EWNwADAAEAAAAAAAAAAAAAAAAACACEAAEAAAAAAAAAAAAAAAAxAAACAAQARAKAAsAAKwBUAAABIiYnJjQ3NzY2MzIWFxYUBwcGIicmNDc3NjQnJiYjIgYHBwYUFxYUBwYGIwciJicmNDc3NjIXFhQHBwYUFxYWMzI2Nzc2NCcmNDc2MhcWFAcHBgYjARQGDAUtLXoWOR8fORYtLTgKGwoKCjgaGg0gEhIgDXoaGgkJBQwHdR85Fi0tOAobCgoKOBoaDSASEiANehoaCQkKGwotLXoWOR8BMwUFLYEuehYXFxYugC44CQkKGwo4GkoaDQ0NDXoaShoKGwoFBe8XFi6ALjgJCQobCjgaShoNDQ0NehpKGgobCgoKLYEuehYXAAAADACWAAEAAAAAAAEACAAAAAEAAAAAAAIAAwAIAAEAAAAAAAMACAAAAAEAAAAAAAQACAAAAAEAAAAAAAUAAQALAAEAAAAAAAYACAAAAAMAAQQJAAEAEAAMAAMAAQQJAAIABgAcAAMAAQQJAAMAEAAMAAMAAQQJAAQAEAAMAAMAAQQJAAUAAgAiAAMAAQQJAAYAEAAMYW5jaG9yanM0MDBAAGEAbgBjAGgAbwByAGoAcwA0ADAAMABAAAAAAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAH//wAP) format("truetype"); }',e.sheet.cssRules.length)}}this.options=A||{},this.elements=[],e(this.options),this.isTouchDevice=function(){return!!("ontouchstart"in window||window.DocumentTouch&&document instanceof DocumentTouch)},this.add=function(A){var i,o,s,c,r,a,h,l,u,d,f,g,p=[];if(e(this.options),"touch"===(g=this.options.visible)&&(g=this.isTouchDevice()?"always":"hover"),A||(A="h2, h3, h4, h5, h6"),0===(i=t(A)).length)return this;for(n(),o=document.querySelectorAll("[id]"),s=[].map.call(o,function(A){return A.id}),r=0;r<i.length;r++)if(this.hasAnchorJSLink(i[r]))p.push(r);else{if(i[r].hasAttribute("id"))c=i[r].getAttribute("id");else if(i[r].hasAttribute("data-anchor-id"))c=i[r].getAttribute("data-anchor-id");else{u=l=this.urlify(i[r].textContent),h=0;do{void 0!==a&&(u=l+"-"+h),a=s.indexOf(u),h+=1}while(-1!==a);a=void 0,s.push(u),i[r].setAttribute("id",u),c=u}d=c.replace(/-/g," "),(f=document.createElement("a")).className="anchorjs-link "+this.options.class,f.href="#"+c,f.setAttribute("aria-label","Anchor link for: "+d),f.setAttribute("data-anchorjs-icon",this.options.icon),"always"===g&&(f.style.opacity="1"),""===this.options.icon&&(f.style.font="1em/1 anchorjs-icons","left"===this.options.placement&&(f.style.lineHeight="inherit")),"left"===this.options.placement?(f.style.position="absolute",f.style.marginLeft="-1em",f.style.paddingRight="0.5em",i[r].insertBefore(f,i[r].firstChild)):(f.style.paddingLeft="0.375em",i[r].appendChild(f))}for(r=0;r<p.length;r++)i.splice(p[r]-r,1);return this.elements=this.elements.concat(i),this},this.remove=function(A){for(var e,n,i=t(A),o=0;o<i.length;o++)(n=i[o].querySelector(".anchorjs-link"))&&(-1!==(e=this.elements.indexOf(i[o]))&&this.elements.splice(e,1),i[o].removeChild(n));return this},this.removeAll=function(){this.remove(this.elements)},this.urlify=function(A){var t=/[& +$,:;=?@"#{}|^~[`%!'<>\]\.\/\(\)\*\\]/g;return this.options.truncate||e(this.options),A.trim().replace(/\'/gi,"").replace(t,"-").replace(/-{2,}/g,"-").substring(0,this.options.truncate).replace(/^-+|-+$/gm,"").toLowerCase()},this.hasAnchorJSLink=function(A){var e=A.firstChild&&(" "+A.firstChild.className+" ").indexOf(" anchorjs-link ")>-1,t=A.lastChild&&(" "+A.lastChild.className+" ").indexOf(" anchorjs-link ")>-1;return e||t||!1}}return A});

// Enable links for selected headers
var anchors = new AnchorJS();
anchors.add("h2, h3, h4")
</script>
<!-- Enable Hypothesis annotation. https://web.hypothes.is/ -->
<script src="https://hypothes.is/embed.js" async></script>
</body>
</html>
